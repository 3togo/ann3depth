#!/bin/bash
#$ -l mem=150M
#$ -cwd
#$ -j y
#$ -v GRID_QUEUES,GRID_PREFIX,MEMORY_RATIO,PS_NODES,WORKERS,CONDAENV,MODEL,STEPS,BATCHSIZE,DATASETS,CKPT_DIR,RUNID,CKPT_FREQ,SUM_FREQ,TIMEOUT
#$ -notify
#$ -o grid_logs/keepalive.$JOB_ID.$HOSTNAME.out

source ./tools/grid/startup.sh

KEEP_ALIVE_INTERVAL=10  # Seconds
TEST_INTERVAL=5  # Minutes
MEMORY_RATIO=${MEMORY_RATIO-0.3}
PREFIX=${GRID_PREFIX-a3dx}
REPORT_FILE=grid_logs/${PREFIX}_done
if [ $(wc -c <<< $GRID_QUEUES) -qt 2 ]; then
    QUEUE=-q $GRID_QUEUES
else
    QUEUE=
fi

function clean_up {
    qdel $(qstat -u $(whoami) | grep "${PREFIX}_[^k]" | cut -f2 -d' ')
}


source activate ${CONDAENV-ann3depth}-CPU
echo Active conda environment: $(conda info --envs | grep \*)


# Determine available hosts.
hosts=($(qstat -f -F h -U $(whoami) | python tools/grid/extract_hosts.py))

echo ${#hosts[@]} hosts available: ${hosts[@]}

# We got enough hosts (or so we hope) so let's determine who will become a worker und who a ps node and store
# a proper cluster spec.
cluster_spec_path=$(python tools/grid/split_resources.py --workers $WORKERS --ps-nodes $PS_NODES ${hosts[@]})

if [ $? -eq 0 ]; then
    echo Stored cluster spec as $cluster_spec_path.csv and as $cluster_spec_path.json
else
    echo Can\'t create cluster spec!
    exit 1
fi


echo Starting processes...
nodes=($(cut -f1 -d',' $cluster_spec_path.csv))
ports=($(cut -f2 -d',' $cluster_spec_path.csv))
jobtype=($(cut -f3 -d',' $cluster_spec_path.csv))
count=0
task_worker=0
task_ps=0
task_index=0
for node in "${nodes[@]}"; do
    job=${jobtype[$count]}

    # Calculate how much memory to request
    availablememory=$(qhost -h $node -F mem | tail -n1 | cut -d' ' -f12 | cut -d= -f2)
    memory=$(python tools/grid/calculate_memory.py $availablememory $MEMORY_RATIO)
    param="-l mem=$memory"
    cuda=

    # Calculate how many cuda_cores to request
    if [ "$job" == "worker" ]; then
        cores=$(qhost -h $node -F cuda_cores | tail -n1 | cut -d' ' -f12 | cut -d= -f2 | cut -d. -f1)
        param="$param -l cuda=1 -l cuda_cores=$cores"
        task_index=$task_worker
        cuda=0
        (( task_worker++ ))
    else
        task_index=$task_ps
        cuda=-1
        (( task_ps++ ))
    fi

    # Submit job
    echo Using $node as $job with parameters $param .
    CUDA_VISIBLE_DEVICES=$cuda REPORT_FILE=$REPORT_FILE CLUSTER_SPEC=$cluster_spec_path JOB_TYPE=$job TASK_INDEX=$task_index qsub -w w -l h=$node $param ${QUEUE} -N ${PREFIX}_${job:0:2}_${task_index} tools/grid/distributed_client.sge

    (( count++ ))
done

echo All jobs submitted, checking back in 1 minute to query their status.

sleep 60
if [ $(qstat | grep "${PREFIX}_[^k]" | wc -l) -ne $(($WORKERS + $PS_NODES)) ]; then
    echo Not all jobs worked properly. Cleaning up queue.
    clean_up
    make distributed
    exit
fi

echo All jobs seem to be fine, idling now.


num_jobs=$(qstat | grep "${PREFIX}_[^k]" | wc -l)
alive=$num_jobs
while [ $alive -ne 0 ]; do
    sleep $KEEP_ALIVE_INTERVAL
    alive=$(qstat | grep "${PREFIX}_[^k]" | wc -l)
    if [ $alive -ne $num_jobs ]; then
        echo Some jobs died! Cleaning up queue.
        clean_up
    fi
done
echo All jobs vanished from queue.


# Check if any worker reported finishing and potentially restart.
if [ -f "$REPORT_FILE" ]; then
    echo Job done!
    echo Removing report file...
    rm $REPORT_FILE
    exit 0
fi

echo Resubmitting job
make distributed
